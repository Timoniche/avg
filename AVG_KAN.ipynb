{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qtq8lcNgnvEn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "b7689a8e-bfe5-4bb5-d816-10b0c0ed0c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 591\n",
            "2 677\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1c052f58d873>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mtor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAUB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Receive reward and next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mrsample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_standard_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_batch_mv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m_batch_mv\u001b[0;34m(bmat, bvec)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mjust\u001b[0m \u001b[0mones\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbroadcasted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Actor critic agent\n",
        "# Continuous 2D\n",
        "%reset -f\n",
        "\n",
        "import torch as tor\n",
        "import matplotlib.pyplot as plt\n",
        "# Problem\n",
        "tor.manual_seed(3)\n",
        "LB = tor.tensor([[-1., -1.]]); UB = tor.tensor([[1., 1.]])\n",
        "ALB = 0.1*tor.tensor([[-.1, -.1]]); AUB = tor.tensor([[.1, .1]])\n",
        "dt = 1\n",
        "# Agent\n",
        "nhid = 10\n",
        "alpha = 0.0003\n",
        "actor_body = tor.nn.Sequential(tor.nn.Linear(4, nhid), tor.nn.ReLU(),\n",
        "                               tor.nn.Linear(nhid, nhid), tor.nn.ReLU()\n",
        "                               )\n",
        "actor_mean = tor.nn.Sequential(tor.nn.Linear(nhid, 2))\n",
        "actor_mean[-1].weight.data[:] = 0; actor_mean[-1].bias.data[:] = 0\n",
        "actor_lsigma = tor.nn.Sequential(tor.nn.Linear(nhid, 2))\n",
        "actor_lsigma[-1].weight.data[:] = 0; actor_lsigma[-1].bias.data[:] = 0\n",
        "critic = tor.nn.Sequential(tor.nn.Linear(4, nhid), tor.nn.ReLU(),\n",
        "                           tor.nn.Linear(nhid, nhid), tor.nn.ReLU(),\n",
        "                          tor.nn.Linear(nhid, 1))\n",
        "popt = tor.optim.Adam(list(actor_body.parameters())+list(actor_mean.parameters())+list(actor_lsigma.parameters()),lr=alpha)\n",
        "copt = tor.optim.Adam(critic.parameters(), lr=10*alpha)\n",
        "# Experiment\n",
        "EP = 2000\n",
        "rets = []\n",
        "Slogs = []\n",
        "i = 0\n",
        "for ep in range(EP):\n",
        "    Slogs.append([])\n",
        "    pos = tor.rand((1, 2))*(UB-LB) + LB\n",
        "    vel = tor.zeros((1, 2))\n",
        "    S = tor.cat((pos, vel), 1)\n",
        "    Slogs[-1].append(S)\n",
        "    ret = 0\n",
        "    while True:\n",
        "        # Take action\n",
        "        feat = actor_body(S)\n",
        "        mu = actor_mean(feat)\n",
        "        lsigma = actor_lsigma(feat)\n",
        "        try:\n",
        "            pol = tor.distributions.MultivariateNormal(mu, 0.01*tor.diag(tor.exp(lsigma[0])))\n",
        "        except:\n",
        "            print(\"A\")\n",
        "        A = pol.sample()\n",
        "        tor.clamp(A, ALB, AUB)\n",
        "        # Receive reward and next state\n",
        "        pos = pos + vel*dt + 0.5*A*dt**2\n",
        "        vel[pos < LB] = -0.1*vel[pos < LB]; vel[pos > UB] = -0.1*vel[pos > UB]\n",
        "        pos = tor.clamp(pos, LB, UB)\n",
        "        vel += A*dt\n",
        "        SP = tor.cat((pos, vel), 1)\n",
        "        R = -0.01\n",
        "        done = tor.allclose(pos, tor.zeros(2), atol=0.25) and tor.allclose(vel, tor.zeros(2), atol=0.1)\n",
        "        # Learning\n",
        "        vs = critic(S); vsp = critic(SP)\n",
        "        pobj = pol.log_prob(A)*(R + (1-done)*vsp - vs).detach()\n",
        "        ploss = -pobj\n",
        "        closs = (R + (1-done)*vsp.detach() - vs)**2\n",
        "        popt.zero_grad()\n",
        "        ploss.backward()\n",
        "        popt.step()\n",
        "        copt.zero_grad()\n",
        "        closs.backward()\n",
        "        copt.step()\n",
        "        # Log\n",
        "        Slogs[-1].append(SP)\n",
        "        ret += R\n",
        "        # Termination\n",
        "        if done:\n",
        "            rets.append(ret)\n",
        "            i += 1\n",
        "            print(i, len(Slogs[-1]))\n",
        "            break\n",
        "        S = SP\n",
        "# Plotting\n",
        "plt.plot(-100*tor.tensor(rets))\n",
        "plt.figure()\n",
        "colors = [\"tab:blue\", \"tab:green\", \"tab:orange\", \"tab:purple\", \"tab:red\", \"tab:brown\"]\n",
        "for i in range(-min(30, EP), 0):\n",
        "    color = colors[i%len(colors)]\n",
        "    Slog = tor.cat(Slogs[i])\n",
        "    for i in range(Slog.shape[0]-1):\n",
        "        plt.plot(Slog[i:i+2,0], Slog[i:i+2,1], alpha=(i+1)/Slog.shape[0], color=color, marker='.')\n",
        "plt.xlim([LB[0, 0], UB[0, 0]])\n",
        "plt.ylim([LB[0, 1], UB[0, 1]])\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFsvTdOJn9sJ"
      },
      "outputs": [],
      "source": [
        "# Action Value Gradient agent\n",
        "# Continuous 2D\n",
        "%reset -f\n",
        "\n",
        "import torch as tor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Problem\n",
        "tor.manual_seed(3)\n",
        "LB = tor.tensor([[-1., -1.]]); UB = tor.tensor([[1., 1.]])\n",
        "ALB = 0.1*tor.tensor([[-.1, -.1]]); AUB = tor.tensor([[.1, .1]])\n",
        "dt = 1\n",
        "\n",
        "n_timeout = 5000\n",
        "\n",
        "# Agent\n",
        "nhid = 10\n",
        "alpha = 0.0003\n",
        "actor_body = tor.nn.Sequential(tor.nn.Linear(4, nhid), tor.nn.ReLU(),\n",
        "                               tor.nn.Linear(nhid, nhid), tor.nn.ReLU()\n",
        "                               )\n",
        "actor_mean = tor.nn.Sequential(tor.nn.Linear(nhid, 2))\n",
        "actor_mean[-1].weight.data[:] = 0; actor_mean[-1].bias.data[:] = 0\n",
        "actor_lsigma = tor.nn.Sequential(tor.nn.Linear(nhid, 2))\n",
        "actor_lsigma[-1].weight.data[:] = 0; actor_lsigma[-1].bias.data[:] = 0\n",
        "q_net = tor.nn.Sequential(tor.nn.Linear(6, nhid), tor.nn.ReLU(),\n",
        "                           tor.nn.Linear(nhid, nhid), tor.nn.ReLU(),\n",
        "                          tor.nn.Linear(nhid, 1))\n",
        "popt = tor.optim.Adam(list(actor_body.parameters())+list(actor_mean.parameters())+list(actor_lsigma.parameters()),lr=alpha)\n",
        "qopt = tor.optim.Adam(q_net.parameters(), lr=10*alpha)\n",
        "\n",
        "# Experiment\n",
        "EP = 2000\n",
        "rets = []\n",
        "Slogs = []\n",
        "i = 0\n",
        "for ep in range(EP):\n",
        "    Slogs.append([])\n",
        "    pos = tor.rand((1, 2))*(UB-LB) + LB\n",
        "    vel = tor.zeros((1, 2))\n",
        "    S = tor.cat((pos, vel), 1)\n",
        "    Slogs[-1].append(S)\n",
        "    ret = 0\n",
        "    step = 0\n",
        "    while True:\n",
        "        # Take action\n",
        "        feat = actor_body(S)\n",
        "        mu = actor_mean(feat)\n",
        "        lsigma = actor_lsigma(feat)\n",
        "        try:\n",
        "            pol = tor.distributions.MultivariateNormal(mu, 0.01*tor.diag(tor.exp(lsigma[0])))\n",
        "        except:\n",
        "            print(\"A\")\n",
        "        A = pol.sample() # Don't use rsample() here\n",
        "        tor.clamp(A, ALB, AUB)\n",
        "\n",
        "        # Receive reward and next state\n",
        "        pos = pos + vel*dt + 0.5*A*dt**2\n",
        "        vel[pos < LB] = -0.1*vel[pos < LB]; vel[pos > UB] = -0.1*vel[pos > UB]\n",
        "        pos = tor.clamp(pos, LB, UB)\n",
        "        vel += A*dt\n",
        "        SP = tor.cat((pos, vel), 1)\n",
        "        R = -0.01\n",
        "        done = (tor.allclose(pos, tor.zeros(2), atol=0.25) and tor.allclose(vel, tor.zeros(2), atol=0.1)) #or step + 1 == n_timeout\n",
        "\n",
        "        # print(\"Step: {}, \".format(step))\n",
        "\n",
        "        # Learning\n",
        "        q = q_net(tor.cat((S, A), 1))\n",
        "        with tor.no_grad():\n",
        "          featP = actor_body(SP)\n",
        "          muP = actor_mean(featP)\n",
        "          lsigmaP = actor_lsigma(featP)\n",
        "          polP = tor.distributions.MultivariateNormal(muP, 0.01*tor.diag(tor.exp(lsigmaP[0])))\n",
        "          A2 = polP.sample()\n",
        "          q2 = q_net(tor.cat((SP, A2), 1));\n",
        "\n",
        "        # A.requires_grad = False\n",
        "        ## Q loss\n",
        "        qloss = (R + (1-done)*q2 - q)**2\n",
        "\n",
        "        # Policy loss\n",
        "        feat_pi = actor_body(S)\n",
        "        mu_pi = actor_mean(feat_pi)\n",
        "        lsigma_pi = actor_lsigma(feat_pi)\n",
        "        pol_pi = tor.distributions.MultivariateNormal(mu_pi, 0.01*tor.diag(tor.exp(lsigma_pi[0])))\n",
        "        A_pi = pol_pi.rsample()   # Requires rsample()\n",
        "        q_pi = q_net(tor.cat((S, A_pi), 1))\n",
        "        pobj = q_pi\n",
        "        ploss = -pobj\n",
        "\n",
        "\n",
        "        # A.requires_grad = True\n",
        "        popt.zero_grad()\n",
        "        ploss.backward()\n",
        "        popt.step()\n",
        "\n",
        "        qopt.zero_grad()\n",
        "        qloss.backward()\n",
        "        qopt.step()\n",
        "\n",
        "        # Log\n",
        "        Slogs[-1].append(SP)\n",
        "        ret += R\n",
        "        step += 1\n",
        "\n",
        "        # Termination\n",
        "        if done:\n",
        "            rets.append(ret)\n",
        "            i += 1\n",
        "            print(i, len(Slogs[-1]))\n",
        "            break\n",
        "        S = SP\n",
        "\n",
        "# Plotting\n",
        "plt.plot(-100*tor.tensor(rets))\n",
        "plt.figure()\n",
        "colors = [\"tab:blue\", \"tab:green\", \"tab:orange\", \"tab:purple\", \"tab:red\", \"tab:brown\"]\n",
        "for i in range(-min(30, EP), 0):\n",
        "    color = colors[i%len(colors)]\n",
        "    Slog = tor.cat(Slogs[i])\n",
        "    for i in range(Slog.shape[0]-1):\n",
        "        plt.plot(Slog[i:i+2,0], Slog[i:i+2,1], alpha=(i+1)/Slog.shape[0], color=color, marker='.')\n",
        "plt.xlim([LB[0, 0], UB[0, 0]])\n",
        "plt.ylim([LB[0, 1], UB[0, 1]])\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n_tOHxO_O34"
      },
      "source": [
        "**AVG on Challenging Mujoco Benchmark Tasks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw3JxV5N_xPG",
        "outputId": "d9c8cd76-7ef6-44b2-fd99-eaa764266044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mujoco==2.3.6\n",
            "  Downloading mujoco-2.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Collecting gymnasium[mujoco]\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco==2.3.6) (1.4.0)\n",
            "Collecting glfw (from mujoco==2.3.6)\n",
            "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mujoco==2.3.6) (1.26.4)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco==2.3.6) (3.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[mujoco])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.36.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Downloading mujoco-2.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyvirtualdisplay, glfw, farama-notifications, mujoco, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 glfw-2.8.0 gymnasium-1.0.0 mujoco-2.3.6 pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[mujoco] mujoco==2.3.6 pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pykan"
      ],
      "metadata": {
        "id": "Hz1lDg14p0xp",
        "outputId": "bb68a030-57af-4dbd-b21f-8847a3e3cf33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pykan\n",
            "  Downloading pykan-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading pykan-0.2.8-py3-none-any.whl (78 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.1/78.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pykan\n",
            "Successfully installed pykan-0.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, time\n",
        "import argparse, os, traceback\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import gymnasium as gym\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.distributions import MultivariateNormal\n",
        "from gymnasium.wrappers import NormalizeObservation\n",
        "from datetime import datetime\n",
        "from kan import KAN"
      ],
      "metadata": {
        "id": "u8M-4bxMoKE5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def params(model):\n",
        "    return sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "id": "lDRUXo5bosr4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def orthogonal_weight_init(m):\n",
        "    \"\"\" Orthogonal weight initialization for neural networks \"\"\"\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.orthogonal_(m.weight.data)\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "def human_format_numbers(num, use_float=False):\n",
        "    # Make human readable short-forms for large numbers\n",
        "    magnitude = 0\n",
        "    while abs(num) >= 1000:\n",
        "        magnitude += 1\n",
        "        num /= 1000.0\n",
        "    # add more suffixes if you need them\n",
        "    if use_float:\n",
        "        return '%.2f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])\n",
        "    return '%d%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])\n",
        "\n",
        "def set_one_thread():\n",
        "    '''\n",
        "    N.B: Pytorch over-allocates resources and hogs CPU, which makes experiments very slow!\n",
        "    Set number of threads for pytorch to 1 to avoid this issue. This is a temporary workaround.\n",
        "    '''\n",
        "    os.environ['OMP_NUM_THREADS'] = '1'\n",
        "    os.environ['MKL_NUM_THREADS'] = '1'\n",
        "    torch.set_num_threads(1)\n"
      ],
      "metadata": {
        "id": "0d0c1IQUoMnt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    \"\"\" Squashed Normal MLP \"\"\"\n",
        "    def __init__(self, obs_dim, action_dim, device, n_hid):\n",
        "        super(Actor, self).__init__()\n",
        "        self.device = device\n",
        "        self.LOG_STD_MAX = 2\n",
        "        self.LOG_STD_MIN = -20\n",
        "\n",
        "        # Two hidden layers\n",
        "        self.phi = nn.Sequential(\n",
        "            nn.Linear(obs_dim, n_hid),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(n_hid, n_hid),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "        self.mu = nn.Linear(n_hid, action_dim)\n",
        "        self.log_std = nn.Linear(n_hid, action_dim)\n",
        "\n",
        "        self.apply(orthogonal_weight_init)\n",
        "        self.to(device=device)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        phi = self.phi(obs.to(self.device))\n",
        "        phi = phi / torch.norm(phi, dim=1).view((-1, 1))\n",
        "        mu = self.mu(phi)\n",
        "        log_std = self.log_std(phi)\n",
        "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
        "\n",
        "        dist = MultivariateNormal(mu, torch.diag_embed(log_std.exp()))\n",
        "        action_pre = dist.rsample()\n",
        "        lprob = dist.log_prob(action_pre)\n",
        "        lprob -= (2 * (np.log(2) - action_pre - F.softplus(-2 * action_pre))).sum(axis=1)\n",
        "\n",
        "        # N.B: Tanh must be applied _only_ after lprob estimation of dist sampled action!!\n",
        "        #   A mistake here can break learning :/\n",
        "        action = torch.tanh(action_pre)\n",
        "        action_info = {'mu': mu, 'log_std': log_std, 'dist': dist, 'lprob': lprob, 'action_pre': action_pre}\n",
        "\n",
        "        return action, action_info\n"
      ],
      "metadata": {
        "id": "BI1oeuD_oPdx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Q(nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim, device, n_hid):\n",
        "        super(Q, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        # Two hidden layers\n",
        "        self.phi = nn.Sequential(\n",
        "            nn.Linear(obs_dim + action_dim, n_hid),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(n_hid, n_hid),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.q = nn.Linear(n_hid, 1)\n",
        "        self.apply(orthogonal_weight_init)\n",
        "        self.to(device=device)\n",
        "\n",
        "    def forward(self, obs, action):\n",
        "        x = torch.cat((obs, action), -1).to(self.device)\n",
        "        phi = self.phi(x)\n",
        "        phi = phi / torch.norm(phi, dim=1).view((-1, 1))\n",
        "        return self.q(phi).view(-1)"
      ],
      "metadata": {
        "id": "6ZeQBrweoUa5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QFast(nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim, device, n_hid):\n",
        "        super(QFast, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.phi = FastKAN(\n",
        "            layers_hidden=[obs_dim + action_dim, 8, 8],\n",
        "            num_grids=5,\n",
        "        )\n",
        "        self.q = nn.Linear(8, 1)\n",
        "        # self.apply(orthogonal_weight_init)\n",
        "        self.to(device=device)\n",
        "\n",
        "    def forward(self, obs, action):\n",
        "        x = torch.cat((obs, action), -1).to(self.device)\n",
        "        phi = self.phi(x)\n",
        "        phi = phi / torch.norm(phi, dim=1).view((-1, 1))\n",
        "        return self.q(phi).view(-1)"
      ],
      "metadata": {
        "id": "Sp5Mm3b1-VLx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AVG:\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = cfg\n",
        "        self.steps = 0\n",
        "\n",
        "        self.actor = Actor(obs_dim=cfg.obs_dim, action_dim=cfg.action_dim, device=cfg.device, n_hid=cfg.nhid_actor)\n",
        "        print(f'config: {cfg}')\n",
        "        self.Q = QFast(obs_dim=cfg.obs_dim, action_dim=cfg.action_dim, device=cfg.device, n_hid=cfg.nhid_critic)\n",
        "        # self.Q = Q(obs_dim=cfg.obs_dim, action_dim=cfg.action_dim, device=cfg.device, n_hid=cfg.nhid_critic)\n",
        "        # print(f'Q params: {params(self.Q)}')\n",
        "        # todo: move to args\n",
        "        # self.Q = KAN(\n",
        "        #     width=[cfg.obs_dim, cfg.nhid_actor, cfg.action_dim],\n",
        "        #     grid=5,\n",
        "        #     k=3,\n",
        "        #     device=cfg.device,\n",
        "        # )\n",
        "        # print(f'Q_KAN params: {params(self.Q_KAN)}')\n",
        "\n",
        "        self.popt = torch.optim.Adam(self.actor.parameters(), lr=cfg.actor_lr, betas=cfg.betas)\n",
        "        self.qopt = torch.optim.Adam(self.Q.parameters(), lr=cfg.critic_lr, betas=cfg.betas)\n",
        "\n",
        "        self.alpha, self.gamma, self.device = cfg.alpha_lr, cfg.gamma, cfg.device\n",
        "\n",
        "    def compute_action(self, obs):\n",
        "        obs = torch.Tensor(obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
        "        action, action_info = self.actor(obs)\n",
        "        return action, action_info\n",
        "\n",
        "    def update(self, obs, action, next_obs, reward, done, **kwargs):\n",
        "        obs = torch.Tensor(obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
        "        next_obs = torch.Tensor(next_obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
        "        action, lprob = action.to(self.device), kwargs['lprob']\n",
        "\n",
        "        #### Q loss\n",
        "        q = self.Q(obs, action.detach())    # N.B: Gradient should NOT pass through action here\n",
        "        with torch.no_grad():\n",
        "            next_action, action_info = self.actor(next_obs)\n",
        "            next_lprob = action_info['lprob']\n",
        "            q2 = self.Q(next_obs, next_action)\n",
        "            target_V = q2 - self.alpha * next_lprob\n",
        "\n",
        "        delta = reward + (1 - done) *  self.gamma * target_V - q\n",
        "        qloss = delta ** 2\n",
        "        ####\n",
        "\n",
        "        # Policy loss\n",
        "        ploss = self.alpha * lprob - self.Q(obs, action) # N.B: USE reparametrized action\n",
        "        self.popt.zero_grad()\n",
        "        ploss.backward()\n",
        "        self.popt.step()\n",
        "\n",
        "        self.qopt.zero_grad()\n",
        "        qloss.backward()\n",
        "        self.qopt.step()\n",
        "\n",
        "        self.steps += 1\n"
      ],
      "metadata": {
        "id": "Y9QL0A1RoV5R"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "yqf5_jz9_WzY"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    tic = time.time()\n",
        "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + f\"-{args.algo}-{args.env}_seed-{args.seed}\"\n",
        "\n",
        "    # Env\n",
        "    env = gym.make(args.env)\n",
        "    env = NormalizeObservation(env)\n",
        "\n",
        "    #### Reproducibility\n",
        "    env.reset(seed=args.seed)\n",
        "    env.action_space.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "    ####\n",
        "\n",
        "    # Learner\n",
        "    args.obs_dim =  env.observation_space.shape[0]\n",
        "    args.action_dim = env.action_space.shape[0]\n",
        "    agent = AVG(args)\n",
        "\n",
        "    # Interaction\n",
        "    rets, ep_steps = [], []\n",
        "    ret, step = 0, 0\n",
        "    terminated, truncated = False, False\n",
        "    obs, _ = env.reset()\n",
        "    ep_tic = time.time()\n",
        "    try:\n",
        "        for t in range(args.N):\n",
        "            # N.B: Action is a torch.Tensor\n",
        "            action, action_info = agent.compute_action(obs)\n",
        "            sim_action = action.detach().cpu().view(-1).numpy()\n",
        "\n",
        "            # Receive reward and next state\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(sim_action)\n",
        "            agent.update(obs, action, next_obs, reward, terminated, **action_info)\n",
        "            ret += reward\n",
        "            step += 1\n",
        "\n",
        "            obs = next_obs\n",
        "\n",
        "            # Termination\n",
        "            if terminated or truncated:\n",
        "                rets.append(ret)\n",
        "                ep_steps.append(step)\n",
        "                print(\"E: {}| D: {:.3f}| S: {}| R: {:.2f}| T: {}\".format(len(rets), time.time() - ep_tic, step, ret, t))\n",
        "\n",
        "                ep_tic = time.time()\n",
        "                obs, _ = env.reset()\n",
        "                ret, step = 0, 0\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"Exiting this run, storing partial logs in the database for future debugging...\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    if not (terminated or truncated):\n",
        "        # N.B: We're adding a partial episode just to make plotting easier. But this data point shouldn't be used\n",
        "        print(\"Appending partial episode #{}, length: {}, Total Steps: {}\".format(len(rets), step, t+1))\n",
        "        rets.append(ret)\n",
        "        ep_steps.append(step)\n",
        "\n",
        "    # Save returns and args before exiting run\n",
        "    if args.save_model:\n",
        "        agent.save(model_dir=args.results_dir, unique_str=f\"{run_id}_model\")\n",
        "\n",
        "\n",
        "    print(\"Run with id: {} took {:.3f}s!\".format(run_id, time.time()-tic))\n",
        "    return ep_steps, rets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_05mh-DqjuTp",
        "outputId": "1c5d6c6a-9d83-4451-cf97-97378b82c393"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "\n",
        "    args.env = \"Hopper-v4\"\n",
        "    args.seed = 42\n",
        "    # args.N = 10001000\n",
        "    args.N = 1000\n",
        "    args.actor_lr = 0.00006\n",
        "    args.critic_lr = 0.00087\n",
        "    args.gamma = 0.99\n",
        "    args.alpha_lr = 0.6\n",
        "    args.nhid_actor = 256\n",
        "    args.nhid_critic = 256\n",
        "    # Miscellaneous\n",
        "    args.results_dir = \"./results\"\n",
        "    parser.add_argument('--save_model', action='store_true', default=False)\n",
        "\n",
        "    # Adam\n",
        "    args.betas = [0, 0.999]\n",
        "\n",
        "    args.device = device\n",
        "    args.algo = \"AVG\"\n",
        "    args.save_model = False\n",
        "\n",
        "    # Start experiment\n",
        "    # set_one_thread()\n",
        "    ep_steps, rets = main(args)\n",
        "    plt.plot(range(len(rets)), rets)\n",
        "    plt.show()\n",
        "    plt.savefig(f'rewards_{args.env}.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "DRoqTNvWjjxr",
        "outputId": "f3824442-6cbb-472f-c62d-a1b5c9f0de38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'argparse' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c59c33b8ac30>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'argparse' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}